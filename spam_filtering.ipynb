{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing \n",
    "- Removal of stop words 'and', 'the', 'of', etc\n",
    "- Grouping together (lemmatization) different inflected forms of a word like 'include', 'includes', etc\n",
    "- Removing non-words like punctutation marks\n",
    "- We will be looking only at the email content (3rd line) and not subject (1st line)\n",
    "\n",
    "Dataset : https://aclweb.org/aclwiki/Spam_filtering_datasets\n",
    "1. Ling-spam Corpus : A dataset that contains spam messages and messages from the Linguist list. \n",
    "2. Enron-spam : A collection of encrypted datasets that contain spam messages and ham messages from real users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ling-spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict(train_dir):\n",
    "    emails = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]\n",
    "    \n",
    "    all_words = []\n",
    "    \n",
    "    for mail in emails:\n",
    "        with open(mail) as m:\n",
    "            for i, line in enumerate(m):\n",
    "                if i==2:\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "                    \n",
    "    dictionary = Counter(all_words)\n",
    "    \n",
    "    # remove non-words and single characters     \n",
    "    for item in list(dictionary):\n",
    "        if item.isalpha() == False:\n",
    "            del dictionary[item] \n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "            \n",
    "    dictionary = dictionary.most_common(3000)  ## keeping only 3000 most common words in the dict\n",
    "    \n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction :\n",
    "\n",
    "Lets create a feature vector matrix where the numer of rows will be the number of emails in the training set and the number of colums will be the number of words in the dictionary\n",
    "\n",
    "The location 'ij' in the matrix will tell how many time that particular word 'j' appeared in the mail 'i', i.e. its frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most important part of the whole program\n",
    "\n",
    "def extract_feature(train_dir):\n",
    "    files = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]\n",
    "        \n",
    "    feature_matrix = np.zeros((len(files), 3000))\n",
    "    docNum = 0\n",
    "    \n",
    "    for fil in files:\n",
    "        with open(fil) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i==2:  ## content is in line 3\n",
    "                    words = line.split()\n",
    "                    for w in words:\n",
    "                        wordNum = 0\n",
    "                        for i, d in enumerate(dictionary):\n",
    "                            if d[0] == w:\n",
    "                                wordNum = i\n",
    "                                feature_matrix[docNum, wordNum] = words.count(w)\n",
    "            docNum = docNum + 1\n",
    "    \n",
    "    return feature_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = make_dict('ling-spam/train-mails/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 702 mails in train dir \n",
    "train_matrix = extract_feature('ling-spam/train-mails/')\n",
    "\n",
    "train_labels = np.zeros(702)\n",
    "\n",
    "fileNum = 0\n",
    "\n",
    "for file in os.listdir('ling-spam/train-mails/'):\n",
    "    if fnmatch.fnmatch(file, \"spm*txt\"):\n",
    "        train_labels[fileNum] = 1\n",
    "    else:\n",
    "        train_labels[fileNum] = 0\n",
    "    fileNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_matrix = extract_feature('ling-spam/test-mails/')\n",
    "\n",
    "test_labels = np.zeros(260)  ## 260 test mails\n",
    "\n",
    "fileNum = 0\n",
    "\n",
    "for file in os.listdir('ling-spam/test-mails/'):\n",
    "    if fnmatch.fnmatch(file, \"spm*txt\"):\n",
    "        test_labels[fileNum] = 1\n",
    "    else:\n",
    "        test_labels[fileNum] = 0\n",
    "    fileNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training SVM as model1 and Naive Baye's as model2\n",
    "\n",
    "# supervised binary classifiers - effective when high number of features\n",
    "model1 = LinearSVC()\n",
    "model1.fit(train_matrix, train_labels)\n",
    "result1 = model1.predict(test_matrix)\n",
    "print(confusion_matrix(test_labels, result1))\n",
    "print(model1.score(test_matrix, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the popular methods for doc classification, this is a supervised probabilistic classifier - assumes independence btw each feature\n",
    "model2 = MultinomialNB()  \n",
    "model2.fit(train_matrix, train_labels)\n",
    "result2 = model2.predict(test_matrix)\n",
    "print(confusion_matrix(test_labels, result2))\n",
    "print(model2.score(test_matrix, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron-spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make some changes for the different directory structure\n",
    "\n",
    "def make_dict(train_dir):\n",
    "    emails_dirs = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]\n",
    "    \n",
    "    all_words = []\n",
    "    \n",
    "    for email_dir in emails_dirs:\n",
    "        dirs = [os.path.join(email_dir, f) for f in os.listdir(email_dir)]\n",
    "        for d in dirs:\n",
    "            emails = [os.path.join(d, f) for f in os.listdir(d)]\n",
    "            \n",
    "            for mail in emails:\n",
    "                with open(mail, encoding=\"latin-1\") as m:\n",
    "                    for line in m:\n",
    "                            words = line.split()\n",
    "                            all_words += words\n",
    "                    \n",
    "    dictionary = Counter(all_words)\n",
    "    \n",
    "    # remove non-words and single characters     \n",
    "    for item in list(dictionary):\n",
    "        if item.isalpha() == False:\n",
    "            del dictionary[item] \n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "            \n",
    "    dictionary = dictionary.most_common(3000)  ## keeping only 3000 most common words in the dict\n",
    "    np.save('dict_enron.npy', dictionary)\n",
    "    \n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(train_dir):\n",
    "    emails_dirs = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]\n",
    "\n",
    "    feature_matrix = np.zeros((33716, 3000))\n",
    "    docNum = 0\n",
    "    train_labels = np.zeros(33716)\n",
    "\n",
    "    for email_dir in emails_dirs:\n",
    "        dirs = [os.path.join(email_dir, f) for f in os.listdir(email_dir)]\n",
    "        for d in dirs:\n",
    "            emails = [os.path.join(d, f) for f in os.listdir(d)]\n",
    "            for mail in emails:\n",
    "                with open(mail, encoding=\"latin-1\") as m:\n",
    "                    all_words = []\n",
    "                    for line in m:\n",
    "                            words = line.split()\n",
    "                            all_words += words\n",
    "\n",
    "                    for word in all_words:\n",
    "                        wordNum = 0\n",
    "                        for i, d in enumerate(dictionary):\n",
    "                            if d[0] == word:\n",
    "                                wordNum = i\n",
    "                                feature_matrix[docNum, wordNum] = all_words.count(word)\n",
    "\n",
    "                train_labels[docNum] = int(mail.split(\".\")[-2] == 'spam')  ## if spam or not\n",
    "                docNum = docNum + 1\n",
    "\n",
    "    return feature_matrix, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dictionary = make_dict('enron-spam/')\n",
    " \n",
    "train_matrix, labels = extract_feature('enron-spam/')\n",
    "\n",
    "np.save('enron_features.npy', train_matrix)\n",
    "np.save('enron_labels.npy', train_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_matrix, train_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Training SVM as model1 and Naive Baye's as model2\n",
    "\n",
    "# supervised binary classifiers - effective when high number of features\n",
    "model1 = LinearSVC()\n",
    "model1.fit(X_train, y_train)\n",
    "result1 = model1.predict(X_test)\n",
    "print(confusion_matrix(y_test, result1))\n",
    "print(model1.score(X_test, y_test))\n",
    "\n",
    "# one of the popular methods for doc classification, this is a supervised probabilistic classifier - assumes independence btw each feature\n",
    "model2 = MultinomialNB()  \n",
    "model2.fit(X_train, y_train)\n",
    "result2 = model2.predict(X_test)\n",
    "print(confusion_matrix(y_test, result2))\n",
    "print(model1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
